{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data files from the Cornell Movie-Dialogs Corpus\n",
    "corpus_movie_conv = 'cornell movie-dialogs corpus/movie_conversations.txt'\n",
    "corpus_movie_lines = 'cornell movie-dialogs corpus/movie_lines.txt'\n",
    "\n",
    "# Maximum length of sentences to consider\n",
    "max_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file containing movie conversations\n",
    "with open(corpus_movie_conv, 'r') as c:\n",
    "    # Read all lines from the file and store them in a list\n",
    "    conv = c.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file containing individual movie lines\n",
    "with open(corpus_movie_lines, 'r') as l:\n",
    "    # Read all lines from the file and store them in a list\n",
    "    lines = l.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store movie lines\n",
    "lines_dic = {}\n",
    "\n",
    "# Iterate over each line in the lines list\n",
    "for line in lines:\n",
    "    # Split the line into components separated by \" +++$+++ \"\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    \n",
    "    # Map the line's ID (first object) to its text (last object) in the dictionary\n",
    "    lines_dic[objects[0]] = objects[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    # Define a string of punctuation characters to be removed\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    # Initialize an empty string to store the result\n",
    "    no_punct = \"\"\n",
    "    \n",
    "    # Iterate over each character in the input string\n",
    "    for char in string:\n",
    "        # Check if the character is not a punctuation\n",
    "        if char not in punctuations:\n",
    "            # Add the character to the result string if it's not punctuation (space is also a character)\n",
    "            no_punct = no_punct + char  \n",
    "    \n",
    "    # Convert the result string to lowercase and return it\n",
    "    return no_punct.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store question-answer pairs\n",
    "pairs = []\n",
    "\n",
    "# Iterate over each conversation in the conv list\n",
    "for con in conv:\n",
    "    \n",
    "    # Extract the list of line IDs for the conversation\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    \n",
    "    # Iterate over the line IDs to create QA pairs\n",
    "    for i in range(len(ids)):\n",
    "        # Initialize a list to hold a single QA pair\n",
    "        qa_pairs = []\n",
    "        \n",
    "        # Skip the last line since it won't have a following line to pair with\n",
    "        if i==len(ids)-1:\n",
    "            break\n",
    "        \n",
    "        # Process and truncate the first line of the pair\n",
    "        first = remove_punc(lines_dic[ids[i]].strip()) \n",
    "        \n",
    "        # Process and truncate the second line of the pair\n",
    "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
    "        \n",
    "        # Add the processed lines to the qa_pairs list\n",
    "        qa_pairs.append(first.split()[:max_len])\n",
    "        qa_pairs.append(second.split()[:max_len])\n",
    "        \n",
    "        # Add the QA pair to the pairs list\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Counter object to keep track of word frequencies\n",
    "word_freq = Counter()\n",
    "\n",
    "# Iterate over each question-answer pair in the pairs list\n",
    "for pair in pairs:\n",
    "    # Update the word frequency counter with words from the question (first part of the pair)\n",
    "    word_freq.update(pair[0])\n",
    "    \n",
    "    # Update the word frequency counter with words from the answer (second part of the pair)\n",
    "    word_freq.update(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum frequency threshold for words to be included in the vocabulary\n",
    "min_word_freq = 5\n",
    "\n",
    "# Filter out words that occur less frequently than the minimum threshold\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "\n",
    "# Create a word map (dictionary) where each word is assigned a unique integer value\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "\n",
    "# Add a special token '<unk>' for unknown words (words not in the word_map)\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "\n",
    "# Add special tokens for the start and end of a sentence\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "\n",
    "# Add a special token '<pad>' for padding, assigned the integer value 0\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 18243\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of words (unique entries) in the word_map\n",
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file named 'WORDMAP_corpus.json' in write mode\n",
    "with open('WORDMAP_corpus.json', 'w') as j:\n",
    "    # Dump the word_map dictionary into the file as JSON\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(words, word_map):\n",
    "    # Encode each word in the question using the word_map. Use '<unk>' token for unknown words.\n",
    "    # Add padding to the encoded question to ensure it has a consistent length of max_len.\n",
    "    # The number of padding tokens added is max_len minus the length of the question.\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_reply(words, word_map):\n",
    "    # Start the encoding with the '<start>' token\n",
    "    # Encode each word in the reply using the word_map. Use '<unk>' token for unknown words.\n",
    "    # Append the '<end>' token to signify the end of the reply\n",
    "    # Add padding to the encoded reply to ensure it has a consistent length of max_len.\n",
    "    # The number of padding tokens added is max_len minus the length of the reply (including start and end tokens).\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
    "    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    \n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the encoded question-answer pairs\n",
    "pairs_encoded = []\n",
    "\n",
    "# Iterate over each question-answer pair in the pairs list\n",
    "for pair in pairs:\n",
    "    # Encode the question part of the pair\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    \n",
    "    # Encode the answer part of the pair\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    \n",
    "    # Append the encoded question and answer as a pair to the pairs_encoded list\n",
    "    pairs_encoded.append([qus, ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file named 'pairs_encoded.json' in write mode\n",
    "with open('pairs_encoded.json', 'w') as p:\n",
    "    # Serialize and write the pairs_encoded list to the file as JSON\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the word_map to create a reverse mapping from integers to words\n",
    "# rev_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "# Convert the first question (index 0) of the second pair (index 1) in pairs_encoded back to words\n",
    "# ' '.join([rev_word_map[v] for v in pairs_encoded[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Load the encoded question-reply pairs from the JSON file\n",
    "        self.pairs = json.load(open('pairs_encoded.json'))\n",
    "        \n",
    "        # Store the size of the dataset\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # Retrieve the i-th pair from the dataset\n",
    "        # Convert the question and reply from lists of integers to PyTorch tensors\n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "        \n",
    "        # Return the question and reply tensors\n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        # Return the total number of pairs in the dataset\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(Dataset(),\n",
    "                          batch_size=100,   # Set the batch size to 100\n",
    "                          shuffle=True,     # Enable shuffling to randomize the order of the data\n",
    "                          pin_memory=True)  # Pin memory for faster data transfer to CUDA-enabled GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first batch of question-reply pairs from the train_loader\n",
    "# question, reply = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        # Create a mask for subsequent positions (upper triangular matrix)\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    # Create a mask for the question, where non-zero elements are True\n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)  # Reshape for compatibility (batch_size, 1, 1, max_words)\n",
    "    \n",
    "    # Create a mask for the reply input, where non-zero elements are True\n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # Add dimension for batch compatibility (batch_size, 1, max_words)\n",
    "    \n",
    "    # Apply subsequent mask to the reply input mask\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # Reshape for compatibility (batch_size, 1, max_words, max_words)\n",
    "    \n",
    "    # Create a mask for the reply target, where non-zero elements are True\n",
    "    reply_target_mask = reply_target!=0   # (batch_size, max_words)\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Modified to implement Universal Transformer:\"\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_len=50, num_layers=6):\n",
    "        super(Embeddings, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model  # Model dimension\n",
    "        self.dropout = nn.Dropout(0.1)  # Dropout layer for regularization\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)  # Word embedding layer\n",
    "\n",
    "        # Create positional encodings for word positions\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)  # (1, max_len, d_model)\n",
    "\n",
    "        # Create positional encodings for layer indices\n",
    "        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)  \n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        # Function to create positional encodings\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        \n",
    "        for pos in range(max_len):  # for each position of the word\n",
    "            for i in range(0, d_model, 2):  # for each dimension of the each position\n",
    "                \n",
    "                # Apply sine and cosine functions for positional encoding\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)  # Add a batch dimension\n",
    "        \n",
    "        return pe\n",
    "        \n",
    "        \n",
    "    def forward(self, embedding, layer_idx):\n",
    "        # Forward pass of the embedding layer\n",
    "        \n",
    "        if layer_idx == 0:\n",
    "            # Apply word embedding only for the first layer\n",
    "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Add positional encoding for word positions\n",
    "        # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        embedding += self.pe[:, :embedding.size(1)]  \n",
    "\n",
    "        # Add positional encoding for the layer index\n",
    "        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n",
    "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0 # Ensure d_model is divisible by the number of heads\n",
    "        self.d_k = d_model // heads # Dimension of each head\n",
    "        self.heads = heads      # Number of attention heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Linear layers for transforming query, key, and value\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Linear layer for concatenating outputs\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        \n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # Split and transform the query, key, and value\n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # Compute the attention scores\n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        # Apply the mask\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        # Apply softmax to get attention weights\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # Apply the attention weights to the value\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        \n",
    "        # Concatenate the heads and apply the final linear layer\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        \n",
    "        \n",
    "        return interacted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # First fully connected layer from d_model to middle_dim\n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        \n",
    "        # Second fully connected layer from middle_dim back to d_model\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply dropout to the output of the first layer then...\n",
    "        # apply ReLU to the first layer\n",
    "        out = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Apply dropout to the output of the second layer\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multi-head self-attention mechanism\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        \n",
    "        # Position-wise feedforward network\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # Apply multi-head self-attention and then dropout to the output of the self-attention\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        \n",
    "        # Add the input (residual connection) and apply layer normalization\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        \n",
    "        # Apply the feedforward network and then dropout to the output of the feedforward network\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        \n",
    "        # Add the output of the self-attention (residual connection) and apply layer normalization\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        \n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multi-head self-attention mechanism for the decoder\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        \n",
    "        # Multi-head attention mechanism between the decoder and the encoder\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        \n",
    "        # Position-wise feedforward network\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        # Apply self-attention to the decoder embeddings and then apply dropout\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        \n",
    "        # Add the input (residual connection) followed by layer normalization\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        \n",
    "        # Apply attention between the decoder (query) and the encoder (encoded) and then apply dropout\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        \n",
    "        # Add the output of the previous self-attention (residual connection) followed by layer normalization\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        \n",
    "        # Apply the feedforward network and then dropout\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        \n",
    "        # Add the output of the previous attention (residual connection) followed by layer normalization\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Modified to implement Universal Transformer:\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_layers, word_map):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model  # Model dimension\n",
    "        self.num_layers = num_layers  # Number of layers in both the encoder and decoder\n",
    "        self.vocab_size = len(word_map)  # Vocabulary size\n",
    "\n",
    "        # Embedding layer that includes word embeddings and positional encodings\n",
    "        self.embed = Embeddings(self.vocab_size, d_model, num_layers=num_layers)\n",
    "\n",
    "        # Single encoder and decoder layer that will be reused in each layer of the stack\n",
    "        self.encoder = EncoderLayer(d_model, heads)\n",
    "        self.decoder = DecoderLayer(d_model, heads)\n",
    "\n",
    "        # Final linear layer that projects the decoder output to the vocabulary size\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_embeddings, src_mask):\n",
    "        # Encode the source sequence\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Apply embeddings with positional encoding for each layer\n",
    "            src_embeddings = self.embed(src_embeddings, i)\n",
    "            \n",
    "            # Pass through the encoder layer\n",
    "            src_embeddings = self.encoder(src_embeddings, src_mask)\n",
    "            \n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
    "        # Decode the target sequence\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Apply embeddings with positional encoding for each layer\n",
    "            tgt_embeddings = self.embed(tgt_embeddings, i)\n",
    "            \n",
    "            # Pass through the decoder layer\n",
    "            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "            \n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        # Forward pass of the Transformer model:\n",
    "        \n",
    "        # Encode the source words\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        \n",
    "        # Decode the target words\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        \n",
    "        # Apply the final linear layer and log softmax\n",
    "        out = F.log_softmax(self.logit(decoded), dim=2)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        # Initialization of the AdamWarmup class\n",
    "        self.model_size = model_size # Model size parameter, typically the dimensionality of the embeddings\n",
    "        self.warmup_steps = warmup_steps # Number of steps over which to warm up the learning rate\n",
    "        self.optimizer = optimizer  # The optimizer to which this scheduler will be applied\n",
    "        self.current_step = 0 # Initialize the current step count\n",
    "        self.lr = 0     # Initialize the learning rate\n",
    "        \n",
    "    def get_lr(self):\n",
    "        # Calculate the learning rate based on the current step\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function:\n",
    "        \n",
    "        # Increment the step count\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Get the new learning rate\n",
    "        lr = self.get_lr()\n",
    "        \n",
    "        # Update the learning rate for each parameter group in the optimizer\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "            \n",
    "        # Update the class's learning rate attribute\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Perform an optimization step\n",
    "        self.optimizer.step()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        \n",
    "        # Kullback-Leibler divergence loss\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        \n",
    "        # Confidence level for the true label\n",
    "        self.confidence = 1.0 - smooth\n",
    "        \n",
    "        # Smoothing factor\n",
    "        self.smooth = smooth \n",
    "        \n",
    "        # Vocabulary size\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        # Flatten the prediction and target tensors\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # Reshape to 2D (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        \n",
    "        # Convert the mask to float and flatten it\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1) # Reshape mask to 1D (batch_size * max_words)\n",
    "        \n",
    "        # Create a tensor for smoothed labels\n",
    "        labels = prediction.data.clone()   # Clone the prediction tensor\n",
    "        labels.fill_(self.smooth / (self.size - 1)) # Fill with the smoothed value\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence) # Assign confidence to the true label\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(prediction, labels)    # Compute KL divergence loss. (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum() # Apply the mask and average the loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters and the device for training:\n",
    "\n",
    "d_model = 512   # The dimensionality of the model's embeddings and hidden layers\n",
    "heads = 8       # The number of attention heads in the multi-head attention layers\n",
    "num_layers = 3  # The number of layers in both the encoder and decoder of the Transformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else CPU\n",
    "epochs = 10     # The number of epochs for training\n",
    "\n",
    "# Loading the word map from a JSON file\n",
    "with open('WORDMAP_corpus.json', 'r') as j:\n",
    "    word_map = json.load(j)  # Loading the word map from a JSON file\n",
    "\n",
    "# Initializing the Transformer model\n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
    "transformer = transformer.to(device)  # Move the model to the specified device (GPU/CPU)\n",
    "\n",
    "# Setting up the Adam optimizer with specific hyperparameters\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "# Initializing a custom learning rate scheduler with warmup\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "\n",
    "# Initializing the loss function with label smoothing\n",
    "criterion = LossWithLS(len(word_map), 0.1)  # The smoothing factor is set to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    \n",
    "    # Set the transformer model to training mode\n",
    "    transformer.train()\n",
    "    \n",
    "    # To accumulate the total loss\n",
    "    sum_loss = 0\n",
    "    \n",
    "    # To count the total number of samples processed\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate over batches of data in the train_loader\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        samples = question.shape[0]  # Number of samples in the current batch\n",
    "\n",
    "        # Move the data to the specified device (GPU or CPU)\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare the input and target data for the transformer\n",
    "        reply_input = reply[:, :-1] # Exclude the last token for input\n",
    "        reply_target = reply[:, 1:] # Exclude the first token for target\n",
    "\n",
    "        # Create masks for the question and reply input\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Forward pass: compute the predicted output by the transformer\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute the loss between the predicted output and the target\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backpropagation: compute the gradient of the loss with respect to the parameters\n",
    "        transformer_optimizer.optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        transformer_optimizer.step() # Update parameters\n",
    "        \n",
    "        # Update the total loss and sample count\n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        # Print the average loss every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reverse the word map to convert indices back to words\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    \n",
    "    # Set the transformer model to evaluation mode\n",
    "    transformer.eval()\n",
    "    \n",
    "    # Start token for decoding\n",
    "    start_token = word_map['<start>']\n",
    "    \n",
    "    # Encode the input question\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    \n",
    "    # Initialize the sequence with the start token\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    # Greedy decoding loop\n",
    "    for step in range(max_len - 1):\n",
    "        # Get the current sequence length\n",
    "        size = words.shape[1]\n",
    "        \n",
    "        # Create a target mask for the current sequence\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Decode the sequence so far to predict the next word\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        \n",
    "        # Choose the word with the highest probability as the next word\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        # Stop if the end token is generated\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "            \n",
    "        # Append the next word to the sequence\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct the output sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "    \n",
    "    # Filter out the start token and convert indices to words\n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Train the model for number of epochs\n",
    "    train(train_loader, transformer, criterion, epoch)\n",
    "    \n",
    "    # Save the state of the model and optimizer after the epoch\n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth.tar')\n",
    "\n",
    "# Retrieve the Transformer model from the checkpoint\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an infinite loop for the interactive session:\n",
    "while(1):\n",
    "    # Take a question as input from the user\n",
    "    question = input(\"Question: \") \n",
    "    \n",
    "    # If the user types 'quit', exit the loop\n",
    "    if question == 'quit':\n",
    "        break\n",
    "    \n",
    "    # Take the maximum length for the reply as input from the user\n",
    "    max_len = input(\"Maximum Reply Length: \")\n",
    "    \n",
    "    # Convert the question to a list of word indices\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    # The above line processes each word in the question. If a word is in the word_map, its index is used;\n",
    "    # otherwise, the index for '<unk>' (unknown) is used.\n",
    "    \n",
    "    # Convert the list of word indices to a PyTorch tensor and move it to the specified device (GPU/CPU)\n",
    "    # The unsqueeze(0) adds a batch dimension to the tensor, making it compatible with the model's input requirements.\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Create a mask for the question tensor\n",
    "    # This mask is used to ignore padding (zeros) in the question. The additional unsqueeze operations add necessary dimensions.\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1) \n",
    "    \n",
    "    # Generate a reply using the Transformer model\n",
    "    # The evaluate function generates a reply based on the input question and the maximum reply length.\n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    \n",
    "    # Print the generated reply\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
