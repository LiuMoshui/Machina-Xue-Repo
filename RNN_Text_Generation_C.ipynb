{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries for word to index and index to word mappings:\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0  # Initialize index counter\n",
    "        \n",
    "\n",
    "    def add_word(self, word):\n",
    "        # Add a word to the dictionary:\n",
    "        if word not in self.word2idx:\n",
    "            # Assign the current index to the new word and update the mappings:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            \n",
    "            # Increment the index for the next word:\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Return the number of words in the dictionary:\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcess(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize a Dictionary object to handle word-to-index mappings:\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Open the text file and read lines:\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            \n",
    "            # Tokenize each line and add words to the dictionary:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']  # Add an end-of-sentence token\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "\n",
    "        # Create a tensor to hold the indices of all words in the file:\n",
    "        rep_tensor = torch.LongTensor(tokens)\n",
    "        index = 0\n",
    "        \n",
    "        # Re-open the file and convert each word to its index:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    rep_tensor[index] = self.dictionary.word2idx[word]\n",
    "                    index += 1\n",
    "\n",
    "        # Calculate the number of complete batches:\n",
    "        num_batches = rep_tensor.shape[0] // batch_size   \n",
    "        \n",
    "        # Truncate the tensor to fit an exact number of batches:\n",
    "        rep_tensor = rep_tensor[:num_batches * batch_size]\n",
    "        \n",
    "        # Reshape the tensor for batch processing\n",
    "        rep_tensor = rep_tensor.view(batch_size, -1)\n",
    "\n",
    "        return rep_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the word embeddings:\n",
    "embed_size = 128\n",
    "\n",
    "# Number of features in the hidden state of the LSTM:\n",
    "hidden_size = 1024\n",
    "\n",
    "# Number of stacked LSTM layers:\n",
    "num_layers = 1\n",
    "\n",
    "# Number of epochs to train the model:\n",
    "num_epochs = 20\n",
    "\n",
    "# Number of samples per batch to be passed through the network:\n",
    "batch_size = 20\n",
    "\n",
    "# Length of the sequence to be passed through the LSTM:\n",
    "timesteps = 30\n",
    "\n",
    "# Learning rate for the optimizer:\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the TextProcess class:\n",
    "corpus = TextProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text data from 'alice.txt' using the TextProcess instance:\n",
    "rep_tensor = corpus.get_data('alice.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1484])\n"
     ]
    }
   ],
   "source": [
    "# rep_tensor is the tensor that contains the index of all the words. Each row contains 1659 words by default \n",
    "print(rep_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5290\n"
     ]
    }
   ],
   "source": [
    "# Calculate the size of the vocabulary:\n",
    "vocab_size = len(corpus.dictionary)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of batches based on the timesteps\n",
    "num_batches = rep_tensor.shape[1] // timesteps\n",
    "\n",
    "# Print the number of batches\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        # Embedding layer to convert word indices into dense vectors of fixed size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # LSTM layer for learning sequence data\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer that outputs probabilities over the vocabulary\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Embed the input words\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # LSTM forward pass with the initial hidden and cell states\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "\n",
    "        # Reshape the LSTM output for the fully connected layer\n",
    "        # (batch_size*timesteps, hidden_size)\n",
    "        out = out.reshape(out.size(0) * out.size(1), out.size(2))\n",
    "\n",
    "        # Pass the reshaped output through the fully connected layer\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TextGenerator model\n",
    "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for updating model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(states):\n",
    "    \"\"\"\n",
    "If we have a tensor z,'z.detach()' returns a tensor that shares the same storage\n",
    "as 'z', but with the computation history forgotten. It doesn't know anything about how it was computed. \n",
    "In other words, we have broken the tensor z away from its past history.\n",
    "Here, we want to perform truncated backpropagation through time (TBPTT) to prevent backpropagating\n",
    "through the entire history of states, which can be computationally expensive and lead\n",
    "to vanishing or exploding gradients.TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as \n",
    "a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal \n",
    "dependencies that span more than 20 timesteps.\n",
    "    \"\"\"\n",
    "    return [state.detach() for state in states] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chang.LAPTOP-KLP71L1N\\AppData\\Local\\Temp\\ipykernel_23364\\3478736248.py:25: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  clip_grad_norm(model.parameters(), 0.5)  # Clip gradients to prevent exploding gradient problem\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 8.5770\n",
      "Epoch [2/20], Loss: 7.8484\n",
      "Epoch [3/20], Loss: 5.8080\n",
      "Epoch [4/20], Loss: 5.1032\n",
      "Epoch [5/20], Loss: 4.3615\n",
      "Epoch [6/20], Loss: 3.6998\n",
      "Epoch [7/20], Loss: 3.1544\n",
      "Epoch [8/20], Loss: 2.6325\n",
      "Epoch [9/20], Loss: 2.1855\n",
      "Epoch [10/20], Loss: 1.9412\n",
      "Epoch [11/20], Loss: 1.5177\n",
      "Epoch [12/20], Loss: 1.4083\n",
      "Epoch [13/20], Loss: 1.0521\n",
      "Epoch [14/20], Loss: 0.8490\n",
      "Epoch [15/20], Loss: 0.4854\n",
      "Epoch [16/20], Loss: 0.2456\n",
      "Epoch [17/20], Loss: 0.1432\n",
      "Epoch [18/20], Loss: 0.0792\n",
      "Epoch [19/20], Loss: 0.0498\n",
      "Epoch [20/20], Loss: 0.0668\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available and set the device accordingly:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize hidden and cell states to zeros:\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "    # Loop through the dataset in batches of 'timesteps' length\n",
    "    for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n",
    "        # Prepare mini-batch inputs and targets\n",
    "        inputs = rep_tensor[:, i:i+timesteps].to(device)  \n",
    "        targets = rep_tensor[:, (i+1):(i+1)+timesteps].to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        outputs, states = model(inputs, states)\n",
    "        states = detach(states)  # Detach states from the graph to prevent backprop through entire history\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets.reshape(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        model.zero_grad()  # Zero out gradients\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        clip_grad_norm(model.parameters(), 0.5)  # Clip gradients to prevent exploding gradient problem\n",
    "        optimizer.step()  # Update model parameters\n",
    "              \n",
    "        # Print loss every 100 steps\n",
    "        step = (i+1) // timesteps\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/500] words and save to results.txt\n",
      "Sampled [200/500] words and save to results.txt\n",
      "Sampled [300/500] words and save to results.txt\n",
      "Sampled [400/500] words and save to results.txt\n",
      "Sampled [500/500] words and save to results.txt\n"
     ]
    }
   ],
   "source": [
    "# Test the model for text generation:\n",
    "\n",
    "with torch.no_grad():\n",
    "    with open('results.txt', 'w') as f:\n",
    "        # Initialize hidden and cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "\n",
    "        # Select one word id randomly and convert it to shape (1,1)\n",
    "        input = torch.randint(0, vocab_size, (1,)).long().unsqueeze(1).to(device)\n",
    "\n",
    "        # Generate words for the specified number of steps\n",
    "        for i in range(500):\n",
    "            # Forward pass through the model\n",
    "            output, state = model(input, state)\n",
    "\n",
    "            # Sample a word id from the exponential of the output \n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "            # Replace the input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # Convert the word id to the actual word and write results to file\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            # Print progress every 100 words\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
