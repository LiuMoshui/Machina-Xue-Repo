{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNkm02TL0wX_"
   },
   "source": [
    "### Gradient Accumulation\n",
    "\n",
    "- In many situations, we want to have a high batch size (desired batch size), however our GPU can only handle a specific batch size (tolerable batch size). One option is to have multiple GPUs and use distributed data training. But what if only one GPU is available? The solution is **gradient accumulation**.\n",
    "<br>\n",
    "<br>\n",
    "- Gradient accumulation (summation) is performing **multiple** backwards passes **before** updating the parameters. The goal is to have the same model parameters for multiple inputs (batches) and then update the model's parameters based on all these batches, instead of performing an update after every single batch.  So we run each torelarbale batch size individually with the same model parameters and calculate the gradients without updating the model. When the desired batch size is reached, we can then update the gradients. \n",
    "<br>\n",
    "<br>\n",
    "- Point of confusion for sudents. The **computational graph** is automatically destroyed when .backward() is called (unless retain_graph=True is specified), and **NOT** the gradients. The gradients are only reset when calling optimizer.zero_grad()\n",
    "<br>\n",
    "<br>\n",
    "- Let's implement that on a ResNet-101 using Google Colab GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SZFbPs20_3j"
   },
   "outputs": [],
   "source": [
    "# Initialize the ResNet101 model\n",
    "model = torchvision.models.resnet101()\n",
    "\n",
    "# Set the number of iterations for training\n",
    "num_iterations = 10\n",
    "\n",
    "# Initialize the Cross-Entropy loss function\n",
    "xe = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Adam optimizer with a learning rate of 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfTbnNVL1Odo"
   },
   "outputs": [],
   "source": [
    "# Set the batch size:\n",
    "batch_size = 50   # Note: 100 causes issues, possibly due to memory constraints\n",
    "\n",
    "# Training loop:\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Generate random inputs and labels:\n",
    "    inputs = torch.randn(batch_size, 3, 224, 224)  # Random tensor simulating a batch of images\n",
    "    labels = torch.LongTensor(batch_size).random_(0, 100)  # Random labels\n",
    "\n",
    "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = xe(outputs, labels)\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero out the gradients to prevent them from accumulating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"Done one batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBtqQLBx4hzh"
   },
   "outputs": [],
   "source": [
    "# Set the desired and tolerable batch sizes\n",
    "desired_batch_size = 100\n",
    "tolerable_batch_size = 50\n",
    "\n",
    "# Calculate the number of accumulation steps\n",
    "accum_steps = desired_batch_size / tolerable_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation in Model Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plu7srYS1cTl"
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    # Generate a batch of inputs and labels:\n",
    "    inputs = torch.randn(tolerable_batch_size,3,224,224)\n",
    "    labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n",
    "    \n",
    "    # Forward pass and compute loss\n",
    "    loss = xe(model(inputs), labels)\n",
    "    \n",
    "    # The loss needs to be scaled to have the same significance over the whole dataset, because the mean should be taken across\n",
    "    # the whole dataset , which requires the loss to be divided by the number of batches. This is only the case if the loss \n",
    "    # returned is averaged. But if the loss returned is summed (for example: nn.BCELoss(reduction = 'sum')) then we do not need to do this\n",
    "    # -->\n",
    "    \n",
    "    # Scale the loss by the number of accumulation steps:\n",
    "    loss = loss / accum_steps  \n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization step after 'accum_steps' iterations or at the last iteration\n",
    "    if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Done one batch\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gradient accum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
